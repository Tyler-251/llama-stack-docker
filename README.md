# Llama Docker

```serve.bash``` hosts a local ollama runtime with a running model as well as a docker container hosting llama-stack. Python scripts can interact with the ollama model via the llama-stack-client library.  

## Python Dependencies
- llama-stack
- llama-stack-client

## Server Dependencies
- Ollama 
- Docker 
- Machine Requirements
    - Linux OS
    - GPU (recommended)